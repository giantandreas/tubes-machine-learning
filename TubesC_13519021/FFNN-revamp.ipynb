{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weigt:bold;\">TUGAS BESAR A MACHINE LEARNING</h2>\n",
    "\n",
    "- Arjuna Marcelino - 13519021\n",
    "- Sharon Bernadetha Marbun - 13519092\n",
    "- Epata Tuah - 13519120\n",
    "- Giant Andreas Tambunan - 13519127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reader\n",
    "import pandas as pd\n",
    "\n",
    "# Read Model\n",
    "def read_model():\n",
    "    f = pd.read_csv(\"model.csv\")\n",
    "    return f\n",
    "\n",
    "def read_data():\n",
    "    df = pd.read_csv('iris.csv')\n",
    "    # encode species\n",
    "    df['species'] = df['species'].replace(['setosa'],1)\n",
    "    df['species'] = df['species'].replace(['versicolor'],2)\n",
    "    df['species'] = df['species'].replace(['virginica'],3)\n",
    "\n",
    "    # deviding species colloumn into 3 collumn\n",
    "    y = pd.get_dummies(df.species, prefix='Class')\n",
    "    df[\"Class_1\"] = y[\"Class_1\"]\n",
    "    df[\"Class_2\"] = y[\"Class_2\"]\n",
    "    df[\"Class_3\"] = y[\"Class_3\"]\n",
    "    return df\n",
    "\n",
    "# get train and test data\n",
    "# target accessed by access data.Class_1, data.Class_2, data.Class_3 \n",
    "\n",
    "# test = read_data()\n",
    "# train = test.sample(frac=0.9)\n",
    "# test = test.loc[~test.index.isin(train.index)]\n",
    "\n",
    "# for idx, item in train.iterrows():\n",
    "#     print(item[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def linear(x, kwargs=None):\n",
    "    return x\n",
    "\n",
    "\n",
    "    \n",
    "def sigmoid(x):\n",
    "    value = float(1 / (1 + math.exp(x * -1)))\n",
    "    return value\n",
    "    # if value > 0.5:\n",
    "    #     return 1\n",
    "    # else:\n",
    "    #     return 0\n",
    "\n",
    "def relu(x, kwargs):\n",
    "    alpha = kwargs.get(\"alpha\", 0.0)\n",
    "    max_value = kwargs.get(\"max_value\", None)\n",
    "    threshold = 0\n",
    "    if x < threshold:\n",
    "        return max(x, x * alpha)\n",
    "    else:\n",
    "        if max_value == None:\n",
    "            return x\n",
    "        else:\n",
    "            return min(x, max_value)\n",
    "\n",
    "def softmax(arr, kwargs=None):\n",
    "    arr_exp = np.exp(arr)\n",
    "    return arr_exp / arr_exp.sum()\n",
    "\n",
    "def lossDerivative(targetj, oj):\n",
    "    return oj-targetj\n",
    "\n",
    "def lossFunction(targetj, oj, lenOutput=1):\n",
    "    loss = 0\n",
    "    if lenOutput>1:\n",
    "        for i in range(len(targetj)):\n",
    "            for j in range(len(targetj[i])):\n",
    "#                 print(targetj[i])\n",
    "                loss += (targetj[i][j]-oj[i][j]) ** 2\n",
    "    else:\n",
    "        loss += (targetj-oj) * (targetj-oj)\n",
    "    return loss/2\n",
    "\n",
    "def lossSoftmax(pk):\n",
    "    return -1*math.log(pk)\n",
    "\n",
    "def reluDerivative(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def sigmoidDerivative(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "def softmaxDerivative(pj, targetClass=False):\n",
    "    if not targetClass:\n",
    "        return pj\n",
    "    else:\n",
    "        return -1*(1-pj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, value, weight):\n",
    "        self.value = value\n",
    "        self.weight = weight    \n",
    "\n",
    "    def set_value(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def set_weight(self, weight):\n",
    "        self.weight = weight\n",
    "\n",
    "    def get_result(self, prev_layer, activation):\n",
    "        sigma = self.get_sigma(prev_layer=prev_layer)\n",
    "        result = None\n",
    "        if activation == \"sigmoid\":\n",
    "            result = sigmoid(sigma)\n",
    "\n",
    "        elif activation == \"linier\":\n",
    "            result = linear(sigma)\n",
    "            \n",
    "        elif activation == \"relu\":\n",
    "            result = relu(sigma)\n",
    "        \n",
    "        self.set_value(result)\n",
    "    \n",
    "    def get_sigma(self, prev_layer):\n",
    "        i = 0\n",
    "        sigma = 0\n",
    "        for n in prev_layer.neurons:\n",
    "            sigma += n.value * self.weight[i]\n",
    "            i += 1\n",
    "        sigma += prev_layer.bias.value\n",
    "        \n",
    "        return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Class\n",
    "import random\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, idx:int, num_of_neuron:int, activation:str, type:str, prev_layer):\n",
    "        self.bias = Neuron(random.random(), weight=None)\n",
    "        self.neurons = []\n",
    "        self.activation = activation\n",
    "        self.idx = idx\n",
    "        self.type = type\n",
    "        self.prev_layer = prev_layer\n",
    "\n",
    "        if type == \"input\":\n",
    "            for i in range(num_of_neuron):\n",
    "                n = Neuron(0, weight=None)\n",
    "                self.neurons.append(n)\n",
    "\n",
    "        elif type == \"hidden\":\n",
    "            num_prev_neuron = len(prev_layer.neurons)\n",
    "            for i in range(num_of_neuron):\n",
    "                n = Neuron(0, weight=[random.random() for x in range(num_prev_neuron)])\n",
    "                self.neurons.append(n)\n",
    "\n",
    "        elif type == \"output\":\n",
    "            self.bias = None\n",
    "            num_prev_neuron = len(prev_layer.neurons)\n",
    "            for i in range(num_of_neuron):\n",
    "                n = Neuron(0, weight=[random.random() for x in range(num_prev_neuron)])\n",
    "                self.neurons.append(n)\n",
    "\n",
    "\n",
    "    def set_values(self, values):\n",
    "        i = 0\n",
    "        for n in self.neurons:\n",
    "            n.set_value(values[i])\n",
    "            i += 1\n",
    "\n",
    "    def set_neuron_bias(self, biases):\n",
    "        i = 0\n",
    "        for n in self.neurons:\n",
    "            n.bias_weight = biases[i]\n",
    "            i += 1\n",
    "\n",
    "    def set_weights(self, weights:list[list[float]]):\n",
    "        i = 0\n",
    "        for n in self.neurons:\n",
    "            n.set_weight(weights[i])\n",
    "            i += 1\n",
    "\n",
    "    def get_result(self):\n",
    "        for n in self.neurons:\n",
    "            n.get_result(self.prev_layer, self.activation)\n",
    "\n",
    "    def back_prop(self, detot, learning_rate):\n",
    "        # check if not input layer\n",
    "        if self.type == \"input\":\n",
    "            return\n",
    "        \n",
    "        i = 0\n",
    "        temp_detot = []\n",
    "        for n in self.neurons:\n",
    "            derr_tot = 0\n",
    "            dout_dnet = n.value * (1 - n.value)\n",
    "            for err in detot:\n",
    "                derr_tot += err[i]\n",
    "            temp = derr_tot * dout_dnet\n",
    "            temp_neuron = []\n",
    "\n",
    "            for j in range(len(n.weight)):\n",
    "                dnet_dw = self.prev_layer.neurons[j].value\n",
    "                derr_dw = derr_tot * dout_dnet * dnet_dw\n",
    "                temp_neuron.append(temp * n.weight[j])\n",
    "                # update weight\n",
    "                updated = n.weight[j] - (learning_rate * derr_dw)\n",
    "                n.weight[j] = updated\n",
    "\n",
    "            temp_detot.append(temp_neuron)\n",
    "            i += 1\n",
    "        \n",
    "        self.prev_layer.back_prop(temp_detot, learning_rate)\n",
    "\n",
    "    def print_layer(self):\n",
    "        print(f\"Layer {self.idx} ({self.type})\")\n",
    "        print(f\"activation func = {self.activation}\")\n",
    "        for n in self.neurons:\n",
    "            print(f\"Neuron weight = {n.weight} \\t Neuron value = {n.value}\")\n",
    "        if self.type != \"output\":\n",
    "            print(f\"Bias = {self.bias.value}\")\n",
    "        print(f\"prev layer idx = {self.prev_layer}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN Class\n",
    "\n",
    "class FeedForwardNeuralNetwork:\n",
    "    def __init__(self, model, learning_rate, error_threshold, max_iteration):\n",
    "        self.input_layer = None\n",
    "        self.hidden_layer = []\n",
    "        self.output_layer = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.error_threshold = error_threshold\n",
    "        self.max_itteration = max_iteration\n",
    "\n",
    "        temp_layer = None\n",
    "        for index, items in model.iterrows():\n",
    "            if index == 0:\n",
    "                self.input_layer = Layer(index, items[\"neuron\"], items[\"activation\"], \"input\", prev_layer=None)\n",
    "                temp_layer = self.input_layer\n",
    "            elif index > 0 and index < model.index.stop - 1:\n",
    "                layer = Layer(index, items[\"neuron\"], items[\"activation\"], \"hidden\", prev_layer=temp_layer)\n",
    "                temp_layer = layer\n",
    "                self.hidden_layer.append(layer)\n",
    "            elif index == model.index.stop-1:\n",
    "                self.output_layer = Layer(index, items[\"neuron\"], items[\"activation\"], \"output\", prev_layer=temp_layer)\n",
    "\n",
    "    def forward_propagation(self, input):\n",
    "        # assume len input = len input_layer.neurons\n",
    "        self.input_layer.set_values(input)\n",
    "        for lay in self.hidden_layer:\n",
    "            lay.get_result()\n",
    "        self.output_layer.get_result()\n",
    "        \n",
    "    def backward_propagation(self, target):\n",
    "        i = 0\n",
    "        temp_detot = []\n",
    "        # update output layer weights\n",
    "        for out_n in self.output_layer.neurons:\n",
    "            derr_dout = out_n.value - target[i]\n",
    "            dout_dy = out_n.value * (1- out_n.value)\n",
    "            temp = derr_dout * dout_dy\n",
    "            temp_neuron = []\n",
    "            \n",
    "            for j in range(len(out_n.weight)):\n",
    "                dy_dw = self.output_layer.prev_layer.neurons[j].value\n",
    "                derr_dw = derr_dout * dout_dy * dy_dw\n",
    "                temp_neuron.append(temp * out_n.weight[j]) # for hidden layer\n",
    "                # update\n",
    "                updated = out_n.weight[j] - (self.learning_rate * derr_dw)\n",
    "                out_n.weight[j] = updated\n",
    "            \n",
    "            temp_detot.append(temp_neuron)\n",
    "            i += 1\n",
    "        \n",
    "        # backprop and update for hidden layer\n",
    "        self.output_layer.prev_layer.back_prop(temp_detot, self.learning_rate)\n",
    "        \n",
    "    \n",
    "    def learn(self, data_train):\n",
    "        # fromat data_train harus sesuai dengan data yang diambil dari read_data()  \n",
    "        # iterate for each data\n",
    "        for idx, data in data_train.iterrows():\n",
    "            input = []\n",
    "            # input\n",
    "            for i in range(len(self.input_layer.neurons)):\n",
    "                input.append(data[i])\n",
    "            \n",
    "            # output\n",
    "            target = [data[\"Class_1\"], data[\"Class_2\"], data[\"Class_3\"]]\n",
    "\n",
    "            # set input layer\n",
    "            self.input_layer.set_values(input)\n",
    "            # forward and backward propagation\n",
    "            self.forward_propagation(input)\n",
    "            self.backward_propagation(target)\n",
    "            # print(f\"COST = {self.get_error_cost(target)}\")\n",
    "        \n",
    "\n",
    "    def get_error_cost(self, target):\n",
    "        cost = 0\n",
    "        i = 0\n",
    "        for n in self.output_layer.neurons:\n",
    "            dif = target[i] - n.value\n",
    "            cost += (dif ** 2)/2\n",
    "            i += 1\n",
    "        return cost\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        result = []\n",
    "        num_true = 0\n",
    "        for idx, data in test_data.iterrows():\n",
    "            input = []\n",
    "            # input\n",
    "            for i in range(len(self.input_layer.neurons)):\n",
    "                input.append(data[i])\n",
    "            # output\n",
    "            target = [data[\"Class_1\"], data[\"Class_2\"], data[\"Class_3\"]]\n",
    "\n",
    "            # set input layer\n",
    "            self.input_layer.set_values(input)\n",
    "            # forward propagation\n",
    "            self.forward_propagation(input)\n",
    "            temp = -1\n",
    "            idx = 0\n",
    "            i = 1\n",
    "            for out_n in self.output_layer.neurons:\n",
    "                if temp < out_n.value:\n",
    "                    temp = out_n.value\n",
    "                    idx = i\n",
    "                i += 1\n",
    "            temp_result = []\n",
    "            if idx == 1:\n",
    "                temp_result = [1, 0, 0]\n",
    "            elif idx == 2:\n",
    "                temp_result = [0, 1, 0]\n",
    "            elif idx == 3:\n",
    "                temp_result = [0, 0, 1]\n",
    "            \n",
    "            result.append(temp_result)\n",
    "            # predict\n",
    "            if target == temp_result:\n",
    "                num_true += 1\n",
    "            # print(f'target = {target}')\n",
    "            # print(f'pred = {temp_result}')\n",
    "        return result, num_true/len(result)\n",
    "\n",
    "\n",
    "    def print_neural_network(self):\n",
    "        self.input_layer.print_layer()\n",
    "        for lay in self.hidden_layer:\n",
    "            lay.print_layer()\n",
    "        self.output_layer.print_layer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE LEARN\n",
      "Layer 0 (input)\n",
      "activation func = sigmoid\n",
      "Neuron weight = None \t Neuron value = 0\n",
      "Neuron weight = None \t Neuron value = 0\n",
      "Neuron weight = None \t Neuron value = 0\n",
      "Neuron weight = None \t Neuron value = 0\n",
      "Bias = 0.0708345537486077\n",
      "prev layer idx = None\n",
      "\n",
      "\n",
      "Layer 1 (hidden)\n",
      "activation func = sigmoid\n",
      "Neuron weight = [0.40861873952278227, 0.47681078109986075, 0.37276253051162267, 0.6064246511275798] \t Neuron value = 0\n",
      "Neuron weight = [0.03193928836406523, 0.3796796018528136, 0.7893366097048221, 0.5249210909647657] \t Neuron value = 0\n",
      "Neuron weight = [0.3244024034793851, 0.36984641496922055, 0.7806449190760713, 0.2740175445667691] \t Neuron value = 0\n",
      "Bias = 0.9763989288926418\n",
      "prev layer idx = <__main__.Layer object at 0x000002075BA42B30>\n",
      "\n",
      "\n",
      "Layer 2 (output)\n",
      "activation func = sigmoid\n",
      "Neuron weight = [0.17023580312200615, 0.9926245839929752, 0.1202106312857204] \t Neuron value = 0\n",
      "Neuron weight = [0.8987674490131915, 0.010922987795323635, 0.7487194607269312] \t Neuron value = 0\n",
      "Neuron weight = [0.4678122567423396, 0.4147672455198659, 0.8203868845693779] \t Neuron value = 0\n",
      "prev layer idx = <__main__.Layer object at 0x000002076BF3CC70>\n",
      "\n",
      "\n",
      "AFTER LEARN\n",
      "Layer 0 (input)\n",
      "activation func = sigmoid\n",
      "Neuron weight = None \t Neuron value = 5.9\n",
      "Neuron weight = None \t Neuron value = 3.0\n",
      "Neuron weight = None \t Neuron value = 5.1\n",
      "Neuron weight = None \t Neuron value = 1.8\n",
      "Bias = 0.0708345537486077\n",
      "prev layer idx = None\n",
      "\n",
      "\n",
      "Layer 1 (hidden)\n",
      "activation func = sigmoid\n",
      "Neuron weight = [-0.8362535972936389, -2.2362289239921296, 3.6652177325105684, 2.1315164727290323] \t Neuron value = 0.9999825752421064\n",
      "Neuron weight = [0.7661151178812797, 0.8803713233483803, 0.9867405507406362, 0.552224423080726] \t Neuron value = 0.999998254396582\n",
      "Neuron weight = [-11.41393897053323, -15.787075748857037, 17.645658414286725, 22.393582039137755] \t Neuron value = 0.999999843259299\n",
      "Bias = 0.9763989288926418\n",
      "prev layer idx = <__main__.Layer object at 0x000002075BA42B30>\n",
      "\n",
      "\n",
      "Layer 2 (output)\n",
      "activation func = sigmoid\n",
      "Neuron weight = [-8.900600217753357, 3.421313316945014, -5.709452170373994] \t Neuron value = 3.671860998697482e-05\n",
      "Neuron weight = [8.726754602880623, -5.230532902905533, -7.097560476916506] \t Neuron value = 0.06770320680676663\n",
      "Neuron weight = [0.06883459593657006, -5.833765995607607, 7.447115966202738] \t Neuron value = 0.934390751874914\n",
      "prev layer idx = <__main__.Layer object at 0x000002076BF3CC70>\n",
      "\n",
      "\n",
      "acc = 0.94\n"
     ]
    }
   ],
   "source": [
    "# test main\n",
    "\n",
    "model = read_model()\n",
    "neural_network = FeedForwardNeuralNetwork(model=model, learning_rate=0.2, error_threshold=0.01, max_iteration=1000)\n",
    "print(\"BEFORE LEARN\")\n",
    "neural_network.print_neural_network()\n",
    "\n",
    "data = read_data()\n",
    "train = data.sample(frac=0.9)\n",
    "test = data.loc[~data.index.isin(train.index)]\n",
    "\n",
    "for i in range(1000):\n",
    "    neural_network.learn(data)\n",
    "\n",
    "\n",
    "print(\"AFTER LEARN\")\n",
    "neural_network.print_neural_network()\n",
    "\n",
    "result, acc = neural_network.predict(test_data=data)\n",
    "print(f\"acc = {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 1 1 1 0 1 1 1 1 1 1 1]\n",
      "[2 1 1 0 1 2 2 0 2 2 1 1 1 2 2]\n",
      "score = 0.5333333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2076bf3c880>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pembelajaran pake sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.1)\n",
    "\n",
    "mlpclassifier = MLPClassifier(  hidden_layer_sizes=1,\n",
    "                                learning_rate=\"constant\",\n",
    "                                activation=\"logistic\",\n",
    "                                solver=\"sgd\",\n",
    "                                batch_size=2,\n",
    "                                learning_rate_init=0.1,\n",
    "                                tol=0.1,\n",
    "                                max_iter=200)\n",
    "\n",
    "mlpclassifier.fit(X_train, y_train)\n",
    "\n",
    "pred = mlpclassifier.predict(X_test)\n",
    "print(pred)\n",
    "print(y_test)\n",
    "\n",
    "print(f'score = {accuracy_score(pred, y_test)}')\n",
    "cm = confusion_matrix(pred, y_test)\n",
    "cm_display = ConfusionMatrixDisplay(cm)\n",
    "cm_display.plot()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87624cbd01e71ab47bd8d90f30cc74e4be12813511688651a34b6aa52844c5e5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
